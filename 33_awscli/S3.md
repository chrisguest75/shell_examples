# S3

Demonstrate how to use the `awscli` with S3  

TODO:

* generate crc example `aws s3api head-object --bucket chris-test-bucket-444 --key test/random.bin --checksum-mode Enabled --query ChecksumCRC32 --output text`
* New – Simplify Access Management for Data Stored in Amazon S3 [here](https://aws.amazon.com/blogs/aws/new-simplify-access-management-for-data-stored-in-amazon-s3/)
* Bucket policy examples [here](https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html)
* Configuring Amazon S3 Inventory [here](https://docs.aws.amazon.com/AmazonS3/latest/userguide/configure-inventory.html)


## Table of contents

- [S3](#s3)
  - [Table of contents](#table-of-contents)
  - [Background](#background)
  - [Configure](#configure)
  - [Create](#create)
  - [Listing buckets and metadata](#listing-buckets-and-metadata)
  - [Bucket Logging](#bucket-logging)
  - [Listing Contents](#listing-contents)
    - [Using awscli](#using-awscli)
    - [Using curl](#using-curl)
  - [Permissions](#permissions)
    - [Public List](#public-list)
    - [Listing Permissions](#listing-permissions)
  - [Probing for public access and responses](#probing-for-public-access-and-responses)
  - [Create Public Bucket](#create-public-bucket)
  - [Copying](#copying)
  - [Extract direct to S3](#extract-direct-to-s3)
  - [CORS](#cors)
  - [Syncing](#syncing)
  - [Presigning](#presigning)
  - [Deleting](#deleting)
  - [Validating Checksums (single files)](#validating-checksums-single-files)
  - [Generate S3 manifest](#generate-s3-manifest)
  - [Performance](#performance)
  - [Storage Lens](#storage-lens)
  - [Resources](#resources)

## Background

S3 (Simple Storage Service) is a web-based object storage service provided by Amazon Web Services (AWS). It is designed to store and retrieve any amount of data from anywhere on the web. S3 offers a scalable and highly durable platform that enables users to store and retrieve data from any location, using a simple web services interface.  

S3 provides a range of storage classes that allow users to choose the type of storage that best suits their needs based on factors such as access frequency, durability, and cost. S3 also provides features like access control, encryption, and versioning to ensure the security and integrity of stored data.  

S3 is widely used for storing and retrieving data for a variety of applications, including backup and disaster recovery, big data analytics, content distribution, and mobile and web applications.  

NOTES:

* S3 is a global resource  
* Logging buckets have to be in the same regions  

## Configure

```sh
export AWS_PROFILE=
export AWS_REGION=
```

## Create

```sh
# create a bucket
aws s3 mb s3://chris-test-bucket-444
```

## Listing buckets and metadata

```sh
# list buckets in the account
aws s3 ls

aws s3api list-buckets | jq '.["Buckets"][].Name' --raw-output 

# get tags 
aws s3api list-buckets | jq '.["Buckets"][].Name' --raw-output | while read in; do aws s3api get-bucket-tagging --bucket $in; done

# get audit logging for each bucket.
aws s3api list-buckets | jq '.["Buckets"][].Name' --raw-output | while read in; do aws s3api get-bucket-logging --bucket $in; done

# listing buckets with logging.
aws s3api list-buckets | jq '.["Buckets"][].Name' --raw-output | while read bucket; do _BUCKET_LOGGING=$(aws s3api get-bucket-logging --bucket $bucket | jq -c .) 
if [ -z "$_BUCKET_LOGGING" ]; then
    echo '{"bucket": "'$bucket'", "logging": "N/A"}' | jq -c . 
else
    echo "$_BUCKET_LOGGING" | jq --arg bucket $bucket -c '{bucket: $bucket, logging: ("s3://" + .LoggingEnabled.TargetBucket + "/" + .LoggingEnabled.TargetPrefix)}'
fi; done
```

## Bucket Logging

After the access logs are set up, it might take longer than an hour for all requests to be properly logged and delivered.  

```sh
# look at logging bucket
export AWS_PROFILE=
aws --no-cli-pager --profile $AWS_PROFILE s3api get-bucket-logging --bucket $BUCKET_NAME

aws --no-cli-pager --profile $AWS_PROFILE s3 ls s3://myloggingbucket --recursive --human-readable --summarize
```

## Listing Contents

### Using awscli

```sh
# list contents of bucket 
aws --profile myprofile s3 ls s3://bucket_name

# with summary and human readable file sizes
aws s3 ls s3://chris-test-bucket-444  --recursive --human-readable --summarize
```

### Using curl

You can use curl to iterate a public bucket.  

Parameters are explained:  

* ListObjects [here](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjects.html)
* ListObjectsV2 [here](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectsV2.html)

```sh
# iterating directory (marker is used for paging)
export MARKER=/
export PREFIX=myprefix
export BUCKET_NAME=mybucket
export MAXKEYS=3
export BASE_URL=https://$BUCKET_NAME.s3.eu-west-1.amazonaws.com
# for cloudfront to work you need query parameter forwarding enabled.
#export BASE_URL=https://distribution.cloudfront.net

# loop here (repeat next three lines)
curl -s "$BASE_URL/?prefix=$PREFIX&max-keys=$MAXKEYS&marker=$MARKER" | xmllint --format - > ./out/listing.xml
yq e -oy '.ListBucketResult.Contents[].Key' ./out/listing.xml
MARKER=$(yq e -oy '.ListBucketResult.Contents[-1].Key' ./out/listing.xml)

# download a file to invoke logs
curl -s "$BASE_URL/$MARKER" | file -  
```

## Permissions

### Public List

NOTE: Only under exceptional circumstances should you allow public list permissions.  

NOTE: I'm not sure of the difference between the PublicList permission below and the "Access control list (ACL) -> Everyone (public access) -> List" permission. They both seem to do the same thing. You can see using "get-bucket-acl" or iam permissions.  

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicReadGetObject",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::mybucket/*"
        },
        {
            "Sid": "PublicList",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:ListBucket",
            "Resource": "arn:aws:s3:::mybucket"
        }
    ]
}
```

### Listing Permissions

```sh
# show ownerid
aws --no-cli-pager s3api list-buckets --query Owner.ID

export BUCKET_NAME=mybucket
# ispublic 
aws --no-cli-pager s3api get-bucket-policy-status --bucket ${BUCKET_NAME}

# "Access control list (ACL) -> Everyone (public access) -> List" will show as AllUsers READ here.
aws --no-cli-pager s3api get-bucket-acl --bucket ${BUCKET_NAME}

# Show IAM policy. Should you use ACL or IAM permissions?
aws --no-cli-pager s3api get-bucket-policy --bucket ${BUCKET_NAME} | jq -r .Policy | jq .

# list contents (is this possible if you're signed out of profile)
aws --no-cli-pager s3api list-objects --bucket ${BUCKET_NAME}
aws --no-cli-pager s3api get-public-access-block --bucket ${BUCKET_NAME}

# iterating directory 
export MAXKEYS=3
export BASE_URL=https://$BUCKET_NAME.s3.eu-west-1.amazonaws.com
# for cloudfront to work you need query parameter forwarding enabled.
#export BASE_URL=https://distribution.cloudfront.net

# loop here (repeat next three lines)
curl -s "$BASE_URL/?max-keys=$MAXKEYS" | xmllint --format - 

# find index.html
aws --no-cli-pager s3api list-objects --bucket ${MYBUCKET} --prefix index.html
aws --no-cli-pager s3api get-object-acl --bucket ${MYBUCKET} --key index.html
```

## Probing for public access and responses  

```sh
# check public access for buckets in an account.
export AWS_PROFILE=myprofile
aws s3api list-buckets | jq '.["Buckets"][].Name' --raw-output | while read bucket; do 
    _BUCKET_REGION=$(aws --no-cli-pager s3api get-bucket-location --bucket $bucket | jq -r '.LocationConstraint')

    if [ -z "$_BUCKET_REGION" ]; then
        _BUCKET_REGION=us-east-1
    fi
    if [ "$_BUCKET_REGION" = "null" ]; then
        _BUCKET_REGION=us-east-1
    fi
    echo "----"
    echo "https://${bucket}.s3.${_BUCKET_REGION}.amazonaws.com"
    curl -s "https://${bucket}.s3.${_BUCKET_REGION}.amazonaws.com"
    echo "----"
done
```

## Create Public Bucket

```sh
# TODO: This fails - you need allow public access first before setting this.
export BUCKET_NAME=mybucket
aws s3 mb s3://${BUCKET_NAME}

# enable public access
aws s3api put-public-access-block --bucket $BUCKET_NAME --public-access-block-configuration "BlockPublicAcls=false,IgnorePublicAcls=false,BlockPublicPolicy=false,RestrictPublicBuckets=false"

TEMPFILE=$(mktemp)
cat <<- EOF > ${TEMPFILE}
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicReadGetObject",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::${BUCKET_NAME}/*"
        }
    ]
}
EOF
cat ${TEMPFILE}
aws s3api put-bucket-policy --bucket ${BUCKET_NAME} --policy file://${TEMPFILE}
# ispublic 
aws --no-cli-pager s3api get-bucket-policy-status --bucket ${BUCKET_NAME}
```

## Copying

```sh
# Copy files from s3 into s3 subfolder
aws s3 cp "s3://file.json" "s3://bucket_name/folder/subfolder/ignition_etcd_0.json"

# copy local file to s3
aws --profile myprofile s3 cp ./fragment.json s3://bucket_name/fragment.json

# copying folders 
aws --profile myprofile s3 cp --recursive "s3://bucket/folder" ./folder

# copying folders and enabling public read access
aws --profile myprofile s3 cp --recursive "s3://bucket/folder" ./folder --acl public-read

# concurrent uploads - default is 10 (trying to get a maximum throughput to s3) 
aws configure set default.s3.max_concurrent_requests 20
aws configure get default.s3.max_concurrent_requests 
aws s3 cp ./file s3://mybucket/ --recursive | tee ./uploaded-log.txt
```

## Extract direct to S3

If you don't want to decompress an archive.  It seems to be slow though as it doesn't seem to use concurrent requests.  

```sh
export AWS_PROFILE=myprofile
export AWS_REGION=eu-west-1
BUCKET_NAME=mybucket
aws s3 mb s3://${BUCKET_NAME}

# echo names
tar -xf ./out/test.tar --to-command="echo \$TAR_REALNAME" 

# extract files to directly to s3
tar -xf ./out/test.tar --to-command="aws s3 cp - s3://${BUCKET_NAME}/test/\$TAR_REALNAME" 

# list files
aws s3 ls s3://${BUCKET_NAME}
```

## CORS

```sh
# examine the cors profile for the bucket
aws --profile myprofile s3api get-bucket-cors --bucket "bucket" | jq .
```

## Syncing

Syncing files can be done bi-directionally.

```sh
aws s3 sync s3://bucket_name path/to/target

# copy to public bucket and make available. 
AWS_PROFILE=myprofile aws s3 sync --acl public-read ./myfolder s3://mybucket/myfolder
```

## Presigning

Presigned URLs are temporary URLs that grant time-limited access to an object in Amazon S3 or other cloud storage services. They are generated using AWS credentials and provide limited permission to perform a specific set of operations on the object or resource.  

Presigned URLs are often used in scenarios where an application or user needs to share a resource or object without giving permanent access to the resource. For example, a website may provide a download link to a user for a file stored on Amazon S3. Rather than granting the user direct access to the object, the website can generate a presigned URL that is valid for a specific amount of time. The user can then use the presigned URL to download the file during the specified time period.  

Presigned URLs are often used for secure distribution of content, sharing of large files, and temporary access to resources for specific operations or tasks. They provide a secure and efficient way to share resources with limited permissions and help to ensure that resources are not accessed or modified beyond their intended use.  

```sh
# copy a file to a bucket
aws s3 cp ../BATCH.md s3://bucket

# expiry is in seconds
SIGNEDURL1=$(aws s3 presign bucket/BATCH.md --expires-in 1000)
echo $SIGNEDURL1
SIGNEDURL2=$(aws s3 presign bucket/BATCH.md --expires-in 1000)
echo $SIGNEDURL2
curl -v -s -o /dev/null $SIGNEDURL1
curl -v -s -o /dev/null $SIGNEDURL2
```

## Deleting

```sh
aws s3 rm s3://test-bucket/folder --recursive
```

## Validating Checksums (single files)

```sh
# generate random numbers
mkdir -p ./out

dd if=/dev/urandom of=./out/random.bin bs=1024 count=10
# check md5
md5sum ./out/random.bin > ./out/random.bin.md5
md5sum --check ./out/random.bin.md5 

# copy file to s3
aws s3 cp ./out/random.bin s3://bucket/test/random.bin

# show contents
aws s3 ls s3://bucket/test/    

# check md5 matches the etag.  
aws s3api get-object-attributes --bucket bucket --key test/random.bin --object-attributes Checksum ObjectSize ETag --output json | jq -r '.ETag'
```

## Generate S3 manifest

```sh
aws s3api list-objects --bucket mybucket --prefix testsruntimefolder/1678128187/ --query 'Contents[].{Key: Key}'
```

## Performance

Sharding considerations [WHY A “503: SLOW DOWN” RESPONSE FROM AMAZON S3 CAN ACTUALLY BE GOOD FOR YOU!](https://blog.pollett.co.uk/aws-s3-at-speed)  
Organizing objects using prefixes [here](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-prefixes.html)  
Best practices design patterns: optimizing Amazon S3 performance [here](https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html)  

## Storage Lens

```sh
# describe any parameters on the registry
export ACCOUNT_ID=$(aws --no-cli-pager ecr describe-registry | jq -r .registryId)

aws --no-cli-pager s3control list-storage-lens-configurations --account-id ${ACCOUNT_ID}  

aws --no-cli-pager s3control get-storage-lens-configuration --account-id=${ACCOUNT_ID} --config-id=default-account-dashboard
```

## Resources

* What is Amazon S3? [here](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html)  
* AWS CLI s3 get-object [here](https://docs.aws.amazon.com/cli/latest/reference/s3api/get-object.html)  
* Does aws-cli confirm checksums when uploading files to S3, or do I need to manage that myself? [here](https://stackoverflow.com/questions/26168481/does-aws-cli-confirm-checksums-when-uploading-files-to-s3-or-do-i-need-to-manag)  
* New – Additional Checksum Algorithms for Amazon S3 [here](https://aws.amazon.com/blogs/aws/new-additional-checksum-algorithms-for-amazon-s3/)
* Support S3 additional checksums in high-level S3 commands #6750 [here](https://github.com/aws/aws-cli/issues/6750)  
* Amazon S3 Inventory Reports [here](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html)  
* Using Amazon S3 Storage Lens configurations with the AWS CLI [here](https://docs.aws.amazon.com/AmazonS3/latest/userguide/S3LensConfigurationsCLI.html)
